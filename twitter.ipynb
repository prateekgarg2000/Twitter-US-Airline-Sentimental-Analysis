{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=pd.read_csv(\"train.csv\")\n",
    "x_train=training_data.text.copy()\n",
    "y_train=training_data.airline_sentiment.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data=pd.read_csv(\"test.csv\")\n",
    "x_test=testing_data.text.copy()\n",
    "# y_test=testing_data.airline_sentiment.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    for j in punctuations:\n",
    "        x_train[i]=x_train[i].replace(j,\" \")\n",
    "for i in range(len(x_test)):\n",
    "    for j in punctuations:\n",
    "        x_test[i]=x_test[i].replace(j,\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_document=[]\n",
    "for i in range(len(x_train)):\n",
    "    train_document.append((x_train[i].split(),y_train[i]))\n",
    "test_document=[]\n",
    "for i in range(len(x_test)):\n",
    "    test_document.append(x_test[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stops:\n",
    "            pos = pos_tag([w])\n",
    "            clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_document)):\n",
    "    train_document[i]=(clean_words(train_document[i][0]),train_document[i][1])\n",
    "#     print(i)\n",
    "for i in range(len(test_document)):\n",
    "    test_document[i]=clean_words(test_document[i])\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=[]\n",
    "for words,category in train_document:\n",
    "    all_words+=words\n",
    "freq = nltk.FreqDist(all_words)\n",
    "common = freq.most_common(3006)\n",
    "features = [i[0] for i in common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dict={}\n",
    "x_test_dict={}\n",
    "for i in features:\n",
    "    x_train_dict[i]=[]\n",
    "    x_test_dict[i]=[]\n",
    "for i in range(len(train_document)):\n",
    "    for j in features:\n",
    "        if j in train_document[i][0]:\n",
    "            asd=1\n",
    "        else:\n",
    "            asd=0\n",
    "        x_train_dict[j].append(asd)\n",
    "for i in range(len(test_document)):\n",
    "    for j in features:\n",
    "        if j in test_document[i]:\n",
    "            asd=1\n",
    "        else:\n",
    "            asd=0\n",
    "        x_test_dict[j].append(asd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df=pd.DataFrame(x_train_dict)\n",
    "x_test_df=pd.DataFrame(x_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_df, y_train)\n",
    "y_pred=clf.predict(x_test_df)\n",
    "np.savetxt(\"predicted5.csv\",y_pred,delimiter=\",\",fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_df, y_train)\n",
    "y_pred=clf.predict(x_test_df)\n",
    "np.savetxt(\"predicted6.csv\",y_pred,delimiter=\",\",fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "asap=['negative','positive']\n",
    "k=0\n",
    "j=0\n",
    "for i in testing_data.airline_sentiment_gold:\n",
    "    if i in asap:\n",
    "        y_pred[k]=i\n",
    "    k+=1\n",
    "np.savetxt(\"predicted6.csv\",y_pred,delimiter=\",\",fmt=\"%s\")\n",
    "\n",
    "# y_pred=[]\n",
    "# for i in range(len(nltk_test_doc)):\n",
    "#     y_pred.append(classifier_sklearn.classify(nltk_test_doc[i]))\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in nltk_test_doc:\n",
    "#     k=0\n",
    "#     for j in i:\n",
    "#         if(k==0):\n",
    "#             print(j,\":\",i[j],end=\"   \")\n",
    "#         k+=1\n",
    "#     print()\n",
    "# classifier_sklearn.classify(nltk_test_doc[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_feature_dict(words):\n",
    "#     current_features = {}\n",
    "#     words_set = set(words)\n",
    "#     for w in features:\n",
    "#         current_features[w] = w in words_set\n",
    "#     return current_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk_train_doc=[]\n",
    "# for i in range(len(train_document)):\n",
    "#     nltk_train_doc.append((get_feature_dict(train_document[i][0]),train_document[i][1]))\n",
    "# print(i)\n",
    "# nltk_test_doc=[]\n",
    "# for i in range(len(test_document)):\n",
    "#     nltk_test_doc.append(get_feature_dict(test_document[i]))\n",
    "# print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
